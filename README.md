# ğŸ’„ Beauty Retail Lakehouse: End-to-End Data Engineering

![Status](https://img.shields.io/badge/Status-Completed-success?style=for-the-badge)
![Airflow](https://img.shields.io/badge/Orchestration-Apache%20Airflow-blue?style=for-the-badge&logo=apacheairflow)
![dbt](https://img.shields.io/badge/Transformation-dbt%20Core-orange?style=for-the-badge&logo=dbt)
![Docker](https://img.shields.io/badge/Infrastructure-Docker-2496ED?style=for-the-badge&logo=docker)
![Postgres](https://img.shields.io/badge/Warehouse-PostgreSQL-336791?style=for-the-badge&logo=postgresql)

> **Business Case:** ConstruÃ§Ã£o de uma arquitetura de dados escalÃ¡vel para monitoramento de KPIs logÃ­sticos e classificaÃ§Ã£o de vendas em um e-commerce de beleza.

---

## ğŸ“‹ Sobre o Projeto

Este projeto simula um desafio real de engenharia de dados no varejo (inspirado no cenÃ¡rio do **Grupo BoticÃ¡rio**). O objetivo foi sair de scripts manuais e construir uma **Modern Data Stack** completa, containerizada e automatizada.

O pipeline resolve o problema de **descentralizaÃ§Ã£o de dados**, ingerindo transaÃ§Ãµes brutas, armazenando-as em um Data Lake e transformando-as em tabelas analÃ­ticas confiÃ¡veis para o time de negÃ³cios acompanhar **atrasos de entrega** e **ticket mÃ©dio por cliente**.

---

## ğŸ—ï¸ Arquitetura (Lakehouse)

A soluÃ§Ã£o segue a arquitetura de **MedalhÃ£o (Bronze, Silver, Gold)**, garantindo rastreabilidade e qualidade do dado.

graph LR

    subgraph Ingestao
        A["Gerador de Vendas\n(Python Script)"] -->|Raw CSV| B["MinIO\n(Data Lake)"]
    end

    subgraph Warehousing
        B -->|Copy| C["PostgreSQL\nCamada Bronze"]
    end

    subgraph Transformacao
        C -->|dbt run| D["dbt Core"]
        D -->|SQL + Testes| E["PostgreSQL\nCamada Gold"]
    end

    style A fill:#f9f,stroke:#333
    style B fill:#add8e6,stroke:#333
    style D fill:#ff4500,color:white,stroke:#333
    style E fill:#90ee90,stroke:#333


ğŸ› ï¸ DecisÃµes TÃ©cnicas (Tech Stack)

Componente	Tecnologia	Por que escolhi?
OrquestraÃ§Ã£o	Apache Airflow	Para gerenciar dependÃªncias complexas e retries automÃ¡ticos. O cÃ³digo Ã© definido como DAGs (Python), facilitando versionamento.
Data Lake	MinIO	        Simula o AWS S3 localmente. Permite desacoplar o armazenamento (barato) do processamento.
TransformaÃ§Ã£o	dbt Core	Traz as boas prÃ¡ticas de engenharia de software (testes, modularidade, git) para o SQL.
Infraestrutura	Docker Compose	Garante que o ambiente seja reprodutÃ­vel em qualquer mÃ¡quina (Infrastructure as Code).
Warehouse	PostgreSQL	Banco robusto para servir a camada analÃ­tica final.

âš™ï¸ O Pipeline Detalhado
O fluxo Ã© controlado pela DAG 03_pipeline_boticario_final, que executa as seguintes etapas sequenciais:

1. Extract (Python): SimulaÃ§Ã£o de geraÃ§Ã£o de dados transacionais com variaÃ§Ã£o de cenÃ¡rios (pedidos cancelados, atrasados, etc).

2. Load to Lake (MinIO): O arquivo Ã© persistido no bucket landing-zone. Isso garante que, se o banco cair, o dado bruto estÃ¡ salvo (Disaster Recovery).

3. Load to DW (Postgres): Carregamento da tabela vendas_bronze (Raw).

4. Transform (dbt): * Limpeza de tipos de dados.

 â€¢ CriaÃ§Ã£o da regra de negÃ³cio classe_pedido (Premium/PadrÃ£o).

 â€¢ CriaÃ§Ã£o da flag teve_problema para monitoramento de SLA logÃ­stico.

ğŸ“¸ EvidÃªncias

1. OrquestraÃ§Ã£o com Sucesso (Airflow)
O pipeline completo rodando sem intervenÃ§Ã£o manual.

![Fluxo Airflow](./airflow_graph.png)

2. Modelo de Dados Final (AnalÃ­tico)
A tabela final pronta para ser consumida por ferramentas de BI (Power BI/Metabase).

![Terminal SQL](./resultado_final.png)

ğŸ“‚ Estrutura do RepositÃ³rio
boticario_data_ops/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ transformacao_dbt/  # Projeto dbt isolado
â”‚   â”‚   â”œâ”€â”€ models/         # Regras de negÃ³cio SQL
â”‚   â”‚   â””â”€â”€ dbt_project.yml
â”‚   â””â”€â”€ pipeline_ingestao.py # A DAG do Airflow
â”œâ”€â”€ data/                    # PersistÃªncia local do MinIO
â”œâ”€â”€ Dockerfile               # CustomizaÃ§Ã£o da imagem Airflow
â”œâ”€â”€ docker-compose.yaml      # OrquestraÃ§Ã£o dos containers
â””â”€â”€ README.md

ğŸš€ Como Executar
PrÃ©-requisitos: Docker e Docker Compose instalados.
1. Clone o repositÃ³rio:
git clone [https://github.com/ricardoribs/boticario_data_ops.git](https://github.com/ricardoribs/boticario_data_ops.git)
cd boticario_data_ops

2. Suba o ambiente:
docker-compose up -d --build

3. Acesse as interfaces:
 â€¢ Airflow: http://localhost:8080 (User/Pass: airflow)

 â€¢ MinIO: http://localhost:9001 (User/Pass: minioadmin)

4. Execute: Ative a DAG 03_pipeline_boticario_final e acompanhe o fluxo ficar verde!

ğŸ”® PrÃ³ximos Passos (Melhorias)
 â€¢ [ ] Implementar Great Expectations para validaÃ§Ã£o de qualidade de dados na ingestÃ£o.

 â€¢ [ ] Configurar CI/CD com GitHub Actions.

 â€¢ [ ] Migrar o Data Lake para AWS S3 real (Free Tier).
